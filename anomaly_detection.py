# -*- coding: utf-8 -*-
"""5210_team_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xdYtvtQM_RfTT6W0XJoCvmsuS9x4Vum8

# **Anomaly Detection : Credit Card Fraud Detection**
"""

pip install feature_engine

!pip install sesd

!pip install statsmodels==0.12.1

import os
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score
from feature_engine.encoding import WoEEncoder, RareLabelEncoder
from imblearn.over_sampling import RandomOverSampler, SMOTE
from collections import Counter
from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE
pd.set_option('display.max_columns',50)

"""### Load the Dataset"""

data = pd.read_csv("fraudTrain.csv")
data_test = pd.read_csv("fraudTest.csv")
data.head()

data_test.head()

"""## **EDA and feature engineering**"""

data.shape

data.info()

"""Use shape function to know how many rows and columns in the dataset. And as the output above, we know that there are 555719 rows and 23 columns in the dataset.

Use info() function to get a concise summary of the dataframe. And we get the information that there are three data types (int, float, object) in this dataset.
"""

print(data.select_dtypes('object').columns)
print(data.select_dtypes('int').columns)
print(data.select_dtypes('float').columns)

data.isnull().values.any()

data.isnull().sum()

"""As we can see from the output, there is no missing data in the dataset.

## Add more features

### Age feature
"""

# The difference of now and the birthay
data['dob'] = pd.to_datetime(data['dob'])
data['age'] = np.rint((pd.to_datetime('now') - data['dob']) / np.timedelta64(1, 'Y'))
data['age'] = data['age'].astype(int)

## Test data
data_test['dob'] = pd.to_datetime(data_test['dob'])
data_test['age'] = np.rint((pd.to_datetime('now') - data_test['dob']) / np.timedelta64(1, 'Y'))
data_test['age'] = data_test['age'].astype(int)

"""### Hour feature
Here, we simply extract the hour. Then, we encode transactions done in normal hours 5:00-21:00 as normal (0) and transactions done in abnormal hours 21:00-05:00 as abnormal (1)
"""

data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])
data['hour'] = data.trans_date_trans_time.dt.hour
data['hour'] = data['hour'].astype(int)

### Test data
data_test['trans_date_trans_time'] = pd.to_datetime(data_test['trans_date_trans_time'])
data_test['hour'] = data_test.trans_date_trans_time.dt.hour
data_test['hour'] = data_test['hour'].astype(int)

data['hourEnc'] = 0
data.loc[data.hour < 5,'hourEnc'] = 1
data.loc[data.hour > 21,'hourEnc'] = 1

### Test data
data_test['hourEnc'] = 0
data_test.loc[data.hour < 5,'hourEnc'] = 1
data_test.loc[data.hour > 21,'hourEnc'] = 1

"""### Location feature
Distance from the customer's home location to the merchant's location might be the main reason for fraud.
"""

# The difference of longitude and lattitude of respective columns
data["lat_diff"] = abs(data.lat - data.merch_lat)
data["long_diff"] = abs(data["long"] - data["merch_long"])

# It is estimated that difference between each degree of longitude and lattitude is 110 kilometers (approx)
data["displacement"] = np.sqrt(pow((data["lat_diff"]*110),2) + pow((data["long_diff"]*110),2))

# Divided the 'displacement' into near, far and long distance records
data.loc[(data["displacement"]<45),["location"]] = "Nearby"
data.loc[((data["displacement"]>45) & (data["displacement"]<90)),["location"]] = "Far Away"
data.loc[(data["displacement"]>90),["location"]] = "Long Distance"

data['location']

### Test data
data_test["lat_diff"] = abs(data_test.lat - data_test.merch_lat)
data_test["long_diff"] = abs(data_test["long"] - data_test["merch_long"])
data_test["displacement"] = np.sqrt(pow((data_test["lat_diff"]*110),2) + pow((data_test["long_diff"]*110),2))
data_test.loc[(data_test["displacement"]<45),["location"]] = "Nearby"
data_test.loc[((data_test["displacement"]>45) & (data_test["displacement"]<90)),["location"]] = "Far Away"
data_test.loc[(data_test["displacement"]>90),["location"]] = "Long Distance"

"""### City population segment feature"""

data.loc[(data["city_pop"]<10000),["city_pop_segment"]] = "Less Dense"
data.loc[((data["city_pop"]>10000) & (data["city_pop"]<50000)),["city_pop_segment"]] = "Adequately Dense"
data.loc[(data["city_pop"]>50000),["city_pop_segment"]] = "Densely populated"

### Test data
data_test.loc[(data_test["city_pop"]<10000),["city_pop_segment"]] = "Less Dense"
data_test.loc[((data["city_pop"]>10000) & (data_test["city_pop"]<50000)),["city_pop_segment"]] = "Adequately Dense"
data_test.loc[(data_test["city_pop"]>50000),["city_pop_segment"]] = "Densely populated"

"""## Splitting Attributes

### Transaction
According to the output, we know that the total transactions are 1296675, and there are 7506 fraudulent and 1289169 Non-fraudulent within.
"""

# Checking the number of fraudulent and non-fraudulent
data_yes = data[data.is_fraud==1]
data_no = data[data.is_fraud==0]
print('Total Transactions: {}'.format(len(data)))
print('Fraudulent: {}'.format(len(data_yes)))
print('Non-Fraudulent: {}'.format(len(data_no)))

"""Let's initialize a separate data containing fraud transactions to analyze trends"""

data_fraud = data[data["is_fraud"]==1]

"""### City & State
As we can see below, the fraudulent most happened in **'New York', 'Texas', 'Pennsylvania', 'California' and 'Ohio'**.
"""

data_fraud['state'].value_counts().head(5)

"""And the fraudulent most happened in **'Houston', 'Warren', 'Huntsville', 'Naples' and 'Dallas'**."""

data_fraud['city'].value_counts().head(5)

"""### Hour
The fraudulent most happened in two timeline, **0:00-5:00** and **20:00-24:00**
"""

sns.kdeplot(data_fraud["hour"])
plt.xlim(left = 0,right = 24)
plt.show()

"""### Category
**Offline Grocery** and **Internet Shopping** having highest number of frauds made.
"""

sns.countplot(y=data_fraud.category)
plt.show()

"""Checking the peak hours of fraudulent in **grocery_pos** and **shopping_net** categories"""

plt.figure(figsize=[12,6])
plt.subplot(121)
sns.kdeplot(x=data_fraud[data_fraud.category == "grocery_pos"].hour)
plt.xlim(left=0,right=24)
plt.title(label = "Offline Grocery Transactions Frauds", fontdict = {"color": "Red", "size": 15, "weight" : "5"})
plt.subplot(122)
sns.kdeplot(x=data_fraud[data_fraud.category == "shopping_net"].hour)
plt.title(label = "Internet Shopping Transactions Frauds", fontdict = {"color": "Green", "size": 15, "weight" : "5"})
plt.xlim(left=0,right=24)
plt.show()

"""It can be seen in offline Grocery transactions that majority of transactions were done in night after 12:00 am, and  majority of transactions in Internet Shopping were done between 8:00 pm ro 12:00 am

### Location
We can see that the long-distance from merchant to the customer residence happened fraudulent mostly.
"""

sns.countplot(data_fraud.location)
plt.show()

"""### Gender & Age"""

sns.countplot(data_fraud.gender)
plt.show()

"""Fraud transcation happened equally in both male and female."""

plt.figure(figsize=[12,6])
plt.subplot(121)
sns.kdeplot(x=data_fraud.age)
plt.xlim(left=0,right=100)
plt.title(label = "Age Transactions Frauds Distribution", fontdict = {"color": "Blue", "size": 15, "weight" : "5"})

"""We can see that fraudulent mostly happened in the age of **25-35 and 45-60** yrs old of customers."""

plt.figure(figsize=[12,6])
plt.subplot(121)
sns.kdeplot(x=data_fraud[data_fraud.gender=='M'].age)
plt.xlim(left=0,right=100)
plt.title(label = "Male", fontdict = {"color": "Blue", "size": 15, "weight" : "5"})
plt.subplot(122)
sns.kdeplot(x=data_fraud[data_fraud.gender == "F"].age)
plt.title(label = "Female", fontdict = {"color": "Pink", "size": 15, "weight" : "5"})
plt.xlim(left=0,right=100)
plt.show()

"""We can see that ratio of Male and Female are pretty different after **60 yrs old**.

### Job
"""

data_fraud['job'].value_counts().head(10)

"""Top 10 occupations that most possible to be fraud are:
**Science writer, Licensed conveyancer, Systems developer,Engineer-biomedical, Colour technologist, Therapist- occupational, Comptroller, Research scientist (physical sciences), and Commissioning editor.**

### Amount
"""

sns.kdeplot(data_fraud.amt,hue=data_fraud.gender)
plt.xlim(left=0)
plt.show()

"""From here we can visualize that female faces more losses than male

## Data Cleaning
"""

# delete the redundant words in 'merchant'
data['merchant'] = data['merchant'].str.strip('fraud_')

# drop 'Unnamed' columns as it is the index only and we have index present with us
data = data.drop(columns=["Unnamed: 0"])

# drop 'first', 'last' columns, since we will use cc_num to distinct between customers
data = data.drop(columns=["first","last"])

# drop 'unix_time', 'trans_num' column since we don't need it
data = data.drop(columns=["unix_time", "trans_num"])

# drop 'dob' since we got the 'age' column
data = data.drop(columns=["dob"])

# drop 'lat', 'long, 'merch_lat', 'merch_long', 'lat_diff', 'long_diff' columns since we got the 'location' column
data = data.drop(columns = ["lat","long","merch_lat","merch_long","lat_diff","long_diff"])

# drop 'street','city' and 'zip' column since the 'state' included the information
data = data.drop(columns=["street", "city", "zip"])

# dropping 'city_pop' column as it is of no use now
data = data.drop(columns=["city_pop"])

### Test data
data_test['merchant'] = data_test['merchant'].str.strip('fraud_')
data_test = data_test.drop(columns=["Unnamed: 0"])
data_test = data_test.drop(columns=["first","last"])
data_test = data_test.drop(columns=["unix_time", "trans_num"])
data_test = data_test.drop(columns=["dob"])
data_test = data_test.drop(columns = ["lat","long","merch_lat","merch_long","lat_diff","long_diff"])
data_test = data_test.drop(columns=["street", "city", "zip"])
data_test = data_test.drop(columns=["city_pop"])

data.info()

"""## Label encoding"""

le = LabelEncoder()
data['merchant'] = le.fit_transform(data['merchant'])
data['category'] = le.fit_transform(data['category'])
data['gender'] = le.fit_transform(data['gender'])
data['state'] = le.fit_transform(data['state'])
data['job'] = le.fit_transform(data['job'])
data['location'] = le.fit_transform(data['location'])
data['city_pop_segment'] = le.fit_transform(data['city_pop_segment'])

### Test data
data_test['merchant'] = le.fit_transform(data_test['merchant'])
data_test['category'] = le.fit_transform(data_test['category'])
data_test['gender'] = le.fit_transform(data_test['gender'])
data_test['state'] = le.fit_transform(data_test['state'])
data_test['job'] = le.fit_transform(data_test['job'])
data_test['location'] = le.fit_transform(data_test['location'])
data_test['city_pop_segment'] = le.fit_transform(data_test['city_pop_segment'])

"""## Display correlation heatmaps"""

fig, ax = plt.subplots(figsize=(20,10))
sns.heatmap(data.corr(),annot=True).set_title('Correlation heatmap without generated features')

data.corr()['is_fraud'].abs().sort_values(ascending=False)

data.columns

datacleaned = data

"""##Feature Selection"""

frames = [data, data_test]
dataset = pd.concat(frames)
X = dataset.drop(['is_fraud','trans_date_trans_time'],axis = 1)
Y = dataset['is_fraud']
#X = dataset[['city_pop_segment','location','gender','hourEnc','category','hour','amt','state','age','cc_num','trans_date_trans_time','job','displacement']]

from sklearn.feature_selection import SelectKBest,mutual_info_classif
bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)
fit = bestfeatures.fit(X,Y,)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Feature','Score']
print(featureScores.nlargest(10,'Score'))

data=data[['city_pop_segment','location','gender','hourEnc','category','hour','amt','state','age','cc_num','is_fraud']]
data_test = data_test[['city_pop_segment','location','gender','hourEnc','category','hour','amt','state','age','cc_num','is_fraud']]
frames = [data, data_test]
dataset = pd.concat(frames)
Y = dataset['is_fraud']
X = dataset[['city_pop_segment','location','gender','hourEnc','category','hour','amt','state','age','cc_num']]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.3, random_state = 0)

"""##Modeling

## Isolation Forest, SESD for Anomaly Detection

Create dummy variables and get rid of unique identifiers
"""

iso = datacleaned.drop(columns=["trans_date_trans_time","cc_num", "merchant","job", "state"])
isoF = pd.get_dummies(data=iso,columns=["category"], drop_first=True)

"""normalize the data"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

import altair as alt
alt.renderers.enable('default')

scaler = StandardScaler()
np_scaled = scaler.fit_transform(isoF)
isoT = pd.DataFrame(np_scaled)

# train isolation forest
outliers_fraction = 0.005
ifo = IsolationForest(contamination = outliers_fraction)

ifo.fit(isoT)

isoF['is_fraud1'] = pd.Series(ifo.predict(isoT))

#Visualizing hour time vs the amount to identify anomalies
fig, ax = plt.subplots(figsize = (10, 5))
a = isoF.loc[isoF['is_fraud1'] == -1, ['hour', 'amt']]
ax.plot(isoF['hour'], isoF['amt'],
        color = 'orange', label = 'Normal')

ax.scatter(a['hour'], a['amt'],
           color = 'red', label = 'Anomaly')

plt.legend()
plt.show();

isoF['is_fraud1'].value_counts()

fraud_counter = len(data[data['is_fraud'] == 1])
fraud_counter

print("Accuracy percentage:", 100*list(isoF['is_fraud1']).count(-1)/(fraud_counter))

#dataset was too large, took a sample size for SESD

sesd_data = isoF.head(10000)
sesd_data  = sesd_data.values.flatten()

import numpy as np
import sesd

outliers_indices = sesd.seasonal_esd(sesd_data, periodicity=20, hybrid=True, max_anomalies=10)
for idx in outliers_indices:
    print(f'Anomaly index: {idx}, anomaly value: {sesd_data[idx]}')

"""This approach did not seem to work too well for our dataset as it was a pandas dataframe and this algorithm used numpy. The end results were hard to interpret.

##Logistic Regreesion
"""

logreg = LogisticRegression()
logreg.fit(X_train, Y_train)
glm_pred = logreg.predict(X_test)
score_test = logreg.score(X_test, Y_test)
print(score_test)

#Trying to solve Oversampling
oversample = RandomOverSampler(sampling_strategy='minority')
X_train1, Y_train1 = oversample.fit_resample(X_train, Y_train)
print(Counter(Y_train1))

logreg.fit(X_train1, Y_train1)
glm_pred = logreg.predict(X_test)
score_test = logreg.score(X_test, Y_test)
print(score_test)

#Applying SMOTE

sm = SMOTE(sampling_strategy='minority')
X_train2, Y_train2 = sm.fit_resample(X_train, Y_train)
print(Counter(Y_train2))

logreg.fit(X_train2, Y_train2)
glm_SMOTE_pred = logreg.predict(X_test)
score_GLM_SMOTE = logreg.score(X_test, Y_test)
print(score_GLM_SMOTE)

"""##Decision Trees"""

#Decision Tree with SMOTE
dtree = DecisionTreeClassifier()
dtree.fit(X_train2,Y_train2)
dtree_SMOTE_pred = dtree.predict(X_test)
DTscore_SMOTE_test = dtree.score(X_test, Y_test)
print(DTscore_SMOTE_test)

Dtree_SMOTE_pred=Counter(dtree_SMOTE_pred)
Dtree_SMOTE_pred=Dtree_SMOTE_pred[1]
Dtree_SMOTE_pred

dtree.fit(X_train,Y_train)
dtree_pred = dtree.predict(X_test)
DTscore_test = dtree.score(X_test, Y_test)
print(DTscore_test)

Dtree_pred_caught=Counter(dtree_pred)
Dtree_pred_caught=Dtree_pred_caught[1]
Dtree_pred_caught

glm_SMOTE_caught=Counter(glm_SMOTE_pred)
glm_SMOTE_caught=glm_SMOTE_caught[1]

GLM_Fraud_Caught=Counter(dtree_pred)

GLM_Fraud_Caught = Counter(glm_pred)
GLM_Fraud_Caught = GLM_Fraud_Caught[1]

Nbayes = GaussianNB()
Nbayes.fit(X_train,Y_train)
Nbayes_pred = Nbayes.predict(X_test)
Nbayes_test = Nbayes.score(X_test, Y_test)
print(Nbayes_test)

Nbayes_pred_caught=Counter(Nbayes_pred)
Nbayes_pred_caught=Nbayes_pred_caught[1]
Nbayes_pred_caught

Nbayes.fit(X_train2,Y_train2)
Nbayes_SMOTE_pred = Nbayes.predict(X_test)
Nbayes_SMOTE_test = Nbayes.score(X_test, Y_test)
print(Nbayes_SMOTE_test)

Nbayes_SMOTE_caught=Counter(Nbayes_SMOTE_pred)
Nbayes_SMOTE_caught=Nbayes_SMOTE_caught[1]
Nbayes_SMOTE_caught

Counter(Y_test)

table_columns = []
table = pd.DataFrame(columns = table_columns)

table.loc[0,'Model'] = 'Isolation Forest'
table.loc[0,'Accuracy'] = round(list(isoF['is_fraud1']).count(-1)/(fraud_counter),4)
table.loc[0,'Frand Caught'] = fraud_counter
table.loc[1,'Model'] = 'GLM'
table.loc[1,'Accuracy'] = round(score_test,4)
table.loc[1,'Frand Caught'] = GLM_Fraud_Caught
table.loc[2,'Model'] = 'GLM_SMOTE'
table.loc[2,'Accuracy'] = round(score_GLM_SMOTE,4)
table.loc[2,'Frand Caught'] = glm_SMOTE_caught
table.loc[3,'Model'] = 'Dtree'
table.loc[3,'Accuracy'] = round(DTscore_test,4)
table.loc[3,'Frand Caught'] = Dtree_pred_caught
table.loc[4,'Model'] = 'Dtree_SMOTE'
table.loc[4,'Accuracy'] = round(DTscore_SMOTE_test,4)
table.loc[4,'Frand Caught'] = Dtree_SMOTE_pred
table.loc[5,'Model'] = 'Naive Bayes'
table.loc[5,'Accuracy'] = round(Nbayes_test,4)
table.loc[5,'Frand Caught'] = Nbayes_pred_caught
table.loc[6,'Model'] = 'Naive Bayes_SMOTE'
table.loc[6,'Accuracy'] = round(Nbayes_SMOTE_test,4)
table.loc[6,'Frand Caught'] = Nbayes_SMOTE_caught


table

"""##Accuracy: decision trees with original data > glm > decision trees with balanced sample

##Anomaly detected by decision trees model with original data > decision trees model with balanced data > glm
"""